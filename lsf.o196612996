Sender: LSF System <lsfadmin@eu-g3-015>
Subject: Job 196612996: <popest> in cluster <euler> Exited

Job <popest> was submitted from host <eu-login-22> by user <metzgern> in cluster <euler> at Mon Dec 13 16:07:04 2021
Job was executed on host(s) <8*eu-g3-015>, in queue <gpu.4h>, as user <metzgern> in cluster <euler> at Mon Dec 13 16:07:38 2021
</cluster/home/metzgern> was used as the home directory.
</cluster/work/igp_psr/metzgern/HAC/code/repocode/population_estimation> was used as the working directory.
Started at Mon Dec 13 16:07:38 2021
Terminated at Mon Dec 13 16:09:01 2021
Results reported at Mon Dec 13 16:09:01 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash

#BSUB -W 4:00
#BSUB -n 8
#BSUB -R "rusage[mem=4000,ngpus_excl_p=1]"
#BSUB -R "select[gpu_mtotal0>=5500]"
##BSUB -R "rusage[scratch=12500]"
#BSUB -J "popest"

# job index (set this to your system job variable e.g. for parallel job arrays)
# used to set model_idx and test_fold_idx below.
#index=0   # index=0 --> model_idx=0, test_fold_idx=0
index=$((LSB_JOBINDEX - 1))
val_fold=$(( $index % 5 ))

leave=Clipart

# cp -r /scratch2/Code/stylebias/data/OfficeHome $TMPDIR/
# cp -r /cluster/work/igp_psr/nkalischek/stylebias/data/OfficeHome $TMPDIR/
# cp -r /cluster/work/igp_psr/metzgern/HAC/code/codeJohn main/population_estimation/datasets $TMPDIR/

echo job index: $index
echo leave: $leave
echo val_fold: $val_fold

source HACenv/bin/activate

# load modules
module load gcc/8.2.0 gdal/3.2.0 zlib/1.2.9 eth_proxy hdf5/1.10.1


python superpixel_disagg_model.py   -train uga,rwa,tza,nga,moz,cod \
                                    -train_lvl f,f,f,f,f,c \
                                    -test uga,rwa,tza,nga,moz,cod \
                                    -lr 0.0001 \
                                    -optim adam \
                                    -wr 0.001 \
                                    -adamwr 0. \
                                    -lstep 8000 \
                                    --validation_fold ${val_fold} \
                                    -mm m,m,m,m,d,d \
                                    --loss laplaceNLL \
                                    --custom_sampler_weights 1,1,1
                                    --input_scaling True
                                    --output_scaling True

# python3 train.py --optimizer ADAM \
#                  --scheduler MultiStepLR \
#                  --base_learning_rate 0.00001 \
#                  --max_epochs 400 \

(... more ...)
------------------------------------------------------------

Exited with exit code 127.

Resource usage summary:

    CPU time :                                   12.05 sec.
    Max Memory :                                 2206 MB
    Average Memory :                             1557.40 MB
    Total Requested Memory :                     32000.00 MB
    Delta Memory :                               29794.00 MB
    Max Swap :                                   -
    Max Processes :                              6
    Max Threads :                                21
    Run time :                                   82 sec.
    Turnaround time :                            117 sec.

The output (if any) follows:

job index: -1
leave: Clipart
val_fold: -1
/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
wandb: Currently logged in as: mp20 (use `wandb login --relogin` to force relogin)
wandb: Tracking run with wandb version 0.12.2
wandb: Syncing run stellar-haze-1092
wandb:  View project at https://wandb.ai/nandometzger/HAC
wandb:  View run at https://wandb.ai/nandometzger/HAC/runs/172mkoe5
wandb: Run data is saved locally in /cluster/work/igp_psr/metzgern/HAC/code/repocode/population_estimation/wandb/run-20211213_160824-172mkoe5
wandb: Run `wandb offline` to turn off syncing.
ERROR 4: /cluster/work/igp_psr/metzgern/HAC/dataOtherBuildings/UGA/uga_wpop_regions.tif: No such file or directory

Traceback (most recent call last):
  File "superpixel_disagg_model.py", line 529, in <module>
    main()
  File "superpixel_disagg_model.py", line 503, in main
    superpixel_with_pix_data(
  File "superpixel_disagg_model.py", line 356, in superpixel_with_pix_data
    this_dataset = get_dataset(ds, params, building_features, related_building_features)
  File "superpixel_disagg_model.py", line 51, in get_dataset
    fine_regions = gdal.Open(rst_wp_regions_path).ReadAsArray().astype(np.uint32)
AttributeError: 'NoneType' object has no attribute 'ReadAsArray'
wandb: Waiting for W&B process to finish, PID 25095
wandb: Program failed with code 1.  Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /cluster/work/igp_psr/metzgern/HAC/code/repocode/population_estimation/wandb/run-20211213_160824-172mkoe5/logs/debug.log
wandb: Find internal logs for this run at: /cluster/work/igp_psr/metzgern/HAC/code/repocode/population_estimation/wandb/run-20211213_160824-172mkoe5/logs/debug-internal.log
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced stellar-haze-1092: https://wandb.ai/nandometzger/HAC/runs/172mkoe5

/cluster/shadow/.lsbatch/1639408024.196612996.shell: line 44: --input_scaling: command not found
/cluster/shadow/.lsbatch/1639408024.196612996.shell: line 45: --output_scaling: command not found
