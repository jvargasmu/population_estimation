Sender: LSF System <lsfadmin@eu-g2-01>
Subject: Job 196711554: <popest> in cluster <euler> Exited

Job <popest> was submitted from host <eu-login-37> by user <metzgern> in cluster <euler> at Tue Dec 14 10:31:43 2021
Job was executed on host(s) <eu-g2-01>, in queue <gpu.24h>, as user <metzgern> in cluster <euler> at Tue Dec 14 10:32:00 2021
</cluster/home/metzgern> was used as the home directory.
</cluster/work/igp_psr/metzgern/HAC/code/repocode/population_estimation> was used as the working directory.
Started at Tue Dec 14 10:32:00 2021
Terminated at Tue Dec 14 10:32:30 2021
Results reported at Tue Dec 14 10:32:30 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash

#BSUB -W 24:00
#BSUB -n 1 
#BSUB -o euleroutputs/outfile_%J.%I.txt
#BSUB -R "rusage[mem=1000,ngpus_excl_p=1]"
#BSUB -R "select[gpu_mtotal0>=6000]"
##BSUB -R "rusage[scratch=12500]"
#BSUB -J "popest"

# job index (set this to your system job variable e.g. for parallel job arrays)
# used to set model_idx and test_fold_idx below.
#index=0   # index=0 --> model_idx=0, test_fold_idx=0
index=$((LSB_JOBINDEX - 1))
val_fold=$(( $index % 5 ))

leave=Clipart

# cp -r /scratch2/Code/stylebias/data/OfficeHome $TMPDIR/
# cp -r /cluster/work/igp_psr/nkalischek/stylebias/data/OfficeHome $TMPDIR/
# cp -r /cluster/work/igp_psr/metzgern/HAC/code/codeJohn main/population_estimation/datasets $TMPDIR/

echo job index: $index
echo leave: $leave
echo val_fold: $val_fold

source HACenv/bin/activate

# load modules
module load gcc/8.2.0 gdal/3.2.0 zlib/1.2.9 eth_proxy hdf5/1.10.1


python superpixel_disagg_model.py   -train nga \
                                    -train_lvl f \
                                    -test nga \
                                    -lr 0.0001 \
                                    -optim adam \
                                    -wr 0.001 \
                                    -adamwr 0. \
                                    -lstep 8000 \
                                    --validation_fold ${val_fold} \
                                    --loss laplaceNLL \
                                    -mm m \
                                    --input_scaling True \
                                    --output_scaling True

# python3 train.py --optimizer ADAM \
#                  --scheduler MultiStepLR \
#                  --base_learning_rate 0.00001 \
#                  --max_epochs 400 \

(... more ...)
------------------------------------------------------------

TERM_MEMLIMIT: job killed after reaching LSF memory usage limit.
Exited with exit code 137.

Resource usage summary:

    CPU time :                                   9.00 sec.
    Max Memory :                                 1000 MB
    Average Memory :                             756.67 MB
    Total Requested Memory :                     1000.00 MB
    Delta Memory :                               0.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                5
    Run time :                                   30 sec.
    Turnaround time :                            47 sec.

The output (if any) follows:

job index: -1
leave: Clipart
val_fold: -1
/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/site-packages/torch/distributed/distributed_c10d.py:153: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
wandb: Currently logged in as: mp20 (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.2
wandb: Syncing run genial-morning-1118
wandb:  View project at https://wandb.ai/nandometzger/HAC
wandb:  View run at https://wandb.ai/nandometzger/HAC/runs/3scbv9kz
wandb: Run data is saved locally in /cluster/work/igp_psr/metzgern/HAC/code/repocode/population_estimation/wandb/run-20211214_103220-3scbv9kz
wandb: Run `wandb offline` to turn off syncing.

Preparing dataloader for:  ['nga']
0it [00:00, ?it/s]/cluster/shadow/.lsbatch/1639474303.196711554.shell: line 45: 295610 Killed                  python superpixel_disagg_model.py -train nga -train_lvl f -test nga -lr 0.0001 -optim adam -wr 0.001 -adamwr 0. -lstep 8000 --validation_fold ${val_fold} --loss laplaceNLL -mm m --input_scaling True --output_scaling True
